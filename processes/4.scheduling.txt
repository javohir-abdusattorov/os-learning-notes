
--- About scheduling ---

Scheduler is part of operating system that makes choise to run which process next.

CPU-bound process is process that spend most of its time computing
I/O-bound process is process that spend most of its time waiting for I/O


When to schedule:
    * New process is created - whether to run the parent process or the child process. Since both are in ready state, decision can go either way
    * Process exists - some process must be chosen from the set of ready processes
    * Process blocks on I/O, or some other reason - some process must be chosen from the set of ready processes
    * I/O inturrupt occurs - process waiting for the I/O
    * Clock interrupt - switching between processes


Nonpreemtive scheduling algorithm - picks a process to run and then just lets it run until it blocks, or voluntarily reales CPU
Preemtive scheduling algorithm - picks a process lets it run for a maximium of some fixed time


Scheduling environments:
    * Batch system - processes should run for a long time without interrupts, thus process switches should be few as possible
    * Interactive system - all process should run equally with fair process switches, preemption is needed
    * Real time system - meeting deadlines and predictabilty is needed


Scheduling algorithm goals:
    * All systems:
        * Fairness - giving each process a fair share of the CPU
        * Policy enforcement - seeing that stated policy is carried out
        * Balance - keeping all parts of the system busy

    * Batch systems:
        * Throughput - maximize jobs per hour
        * Turnaround time - minimize time between request and response of process
        * CPU utilization - keep the CPU busy all the time

    * Interactive systems:
        * Response time - respond to requests quickly
        * Proportionality - meet users' expectations

    * Real-time systems:
        * Meeting deadlines - avoid losing data
        * Predictability - avoid quality degredation in multimedia systems


Throughput - the number of jobs per hour that the system completes
Turnaround time - statistically average time from the moment that a job is submitted until the moment it is completed

















--- Scheduling in Batch systems ---

First-come, first-served
    Processes are assigned the CPU in order they request it. There will be a single queue of ready and completed processes
    
Shortest job first
    When several equally important jobs are sitting in the input queue, scheduler picks the shortest job first. Shortest job is optimal only when all the jobs are aviable simultaneously.
    
Shortest remaining time next
    Scheduler always chooses the process whose remaining run time is the shortest. The process run time has to be known in advance
    

















--- Scheduling in Interactive systems ---

Round-robin
    Each process is assigned a time interval, called its "quantum", during which it is allowed to run. After finishing its quantum process blocked and put to end of queue. Scheduler makes the implicit assumption that all process are equally important. The interesting issue with round robin is the length of the quantum. Setting the quantum too short causes too many process switches and lowers the CPU efficency. But setting it too long may cause poor response to short interactive requests.

Priority scheduling
    Each process is assigned a priority, and the runnable rpocess with the highest priority is allowed to run. To prevent high-priority processes from running infinitely, the scheduler may decrease the priority of the currently running process at each clock tick, or some other time interval. If this action causes its priority to drop below that of the next highest process, a process switch occurs. Priorities can be assigned to process statically or dynamically.
    It is often convenient to group processes into priority calsses and use priority scheduling among the classes, but round-robin scheduling within classes
    Priority scheduling has problem of indefinite blocking or starvation, in which low priority task can wait forever. One common solution to this problem is aging, in which priorities of jobs increases the longer they wait. Under this scheme a low-priority job will eventually get its priority raised high enough by time.

Multilevel queue scheduling
    When processes can be categorized, then multiple seperate queues can be established, each implementing whatever scheduling algorithm is most appropriate for that type of job. Scheduling must also be done between queues, that is scheduling one queue to get time relative to other queues. Common options are priority and round-robin. Processes cannot switch queues, exit queue or enter queue.

Multilevel feedback-queue scheduling
    This scheduling is similar to multilevel queue scheduling, except jobs can be moved from one queue to another for these reasons:
        * If characteristics of process is switched between CPU-intesive to I/O intensive. 
        * If process aged, so that process has waited for a long time

Shortest process next
    This scheduling runs shortest processes, and calculates which process is shortest based on past behavior. Calculation is simple, adding the new value to current estimate and divide the sum by 2. The technique of estimating the next value in a series by taking the weighted average of the current measured value and the previous estimate is sometimes called aging.

Guaranteed scheduling
    With n processes running, all things being equal, each one should get 1/n of the CPU cycles. System must keep track of how much CPU each process has had since its creation, then computes the amount of CPU each process is entitled to - ratio of actual CPU time. The algorithm is then run the process with the lowest ration until its ratio has moved above that of its closest competitor.

Lottery scheduling
    Idea is to give processes lottery tickets for system resourses, such as CPU time. When a scheduling decision has to be made, a lottery ticket is chosen at random, and the process holding that ticket gets the resource. More important processes can be given extra tickets, to increase their idds of winning. A process holding a fraction F of the tickets will get about a fraction F of the resource in question. Coorporating processes may exchange tickets if they wish



























--- Scheduling in Real-time systems ---

Real-time systems are generally categorized as:
    * hard real time - there are absolute deadlines that must be met
    * soft real time - missing an occasional deadline is undesirable, but nevertheless tolerable

The events that a real-time system may have to response to can be further categorized as:
    * periodic - they occur at regular intervals
    * aperiodic - they occur unpredictably

Real-time scheduling algorithms can be:
    * static - make scheduling desicions before the system starts. Works only when there is perfect information available in advance about the work to be done and the deadlines that have to be met
    * dynamic - make scheduling desicions at run time, after exection has started
















--- Policy versus Mechanism ---

Problem with scheduling algorithms is they do not accept any input from user processes about scheduling decisions. The solution to this problem is seperate the "scheduling mechanism" from "scheduling policy". This means that the scheduling algorithm is parameterized in some way, but parameters can be filled in by user processes. In this way, the parent can control how its children are scheduled, even though it itself does not do the scheduling
























--- Thread scheduling ---

When threads are used, we have two levels of parallelism present: processes and threads. Scheduling in such systems differs depending on whether user-level threads or kernel-level threads.

Scheduling on user-level threads can be round-robin or priority scheduling. The only constraint is the absence of clock interrupt a thread that has run too long
Scheduling on kernel-level threads will be as process scheduling. Difference between user and kernel level threads is the performance. Doing a thread switch in user-level threads takes a handful of machine instructions. But with kernel-level threads it requires a full context switch, like process switch.

Since the kernel knows that switching from a thread in process A to a thread in process B is more expensive than running a second thread in process A, it can this information into account when making a decision.

Another important factor is that user-level threads can employ an application-specific thread scheduler, meaning it knows what all threads do, which to run next. This strategy maximizes the amount of parallelism in I/O-bound environment. With kernel-level threads, the kernel would never know what each thread does.
In general, application-specific thread schedulers can tune an application bettern than the kernel can.























































